
<hr />
<p>layout: post
title: 《白话大数据与机器学习》笔记
categories:</p>
<ul>
  <li>tags:</li>
  <li>machine learning</li>
  <li>data</li>
</ul>

<hr />

<p>###统计与分布</p>
<ol>
  <li>加和值，平均值，标准差，加权均值，众数，中位数，欧氏距离，曼哈顿距离</li>
  <li>高斯分布（miu，sigma）</li>
  <li>泊松分布（lambda，k）
 描述单位时间内随机事件发生的次数。lambda：单位时间内随机事件的平均发生率
 在一个标准时间里，发生这件事的发生率是lambda次，哪发生k次的概率是泊松分布。</li>
  <li>伯努利分布（p）</li>
</ol>

<!--more-->

<p>###信息论</p>
<ol>
  <li>信息量</li>
  <li>
    <p>香农公式  $C = B* log_2(1+\frac{S}{N})$</p>

    <p>C：信道中信号传输的速度 B：码元速率的极限值（B=2H） S：信号功率 N：噪声功率</p>
  </li>
  <li>
    <p>信息熵：<em>信息杂乱程度的量化描述</em></p>

    <p>$H(x)= -\displaystyle\sum_{i=1}^n p(x_i)log_2 P(x_i)$</p>

    <p>在信息可能有N种情况是，如果每种情况出现的概率相等，那么N越大，信息熵越大。
 在信息可能有N重的情况是，当N一定，那么其中所有情况概率相等时信息熵是最大的，如果有一种情况的概率比其他情况概率大很多，那么信息熵会很小。</p>

    <blockquote>
      <p>信息越确定，越单一，信息熵越小。</p>
    </blockquote>

    <blockquote>
      <p>信息越不确定，越混乱，信息熵越大。</p>
    </blockquote>

    <p>信息增益是特征选择中的一个重要指标，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，该特征越重要。</p>
  </li>
</ol>

<p>###回归</p>

<p>倒退，倒推，由果索因，归纳：当看到大量事实所呈现的样态，推断出原因是如何的；看到大量数字对的样态，推断出他们之间蕴含的关系。</p>

<ol>
  <li>线性回归</li>
  <li>残差分析</li>
</ol>

<p>###聚类</p>
<ol>
  <li>K-Means 算法</li>
  <li>层次聚类</li>
  <li>聚类评估
    <ul>
      <li>聚类趋势：霍普金斯统计量（0.5 vs 1）</li>
      <li>簇数确定：$\sqrt{\frac{n}{2}}$ ; 肘方法</li>
    </ul>
  </li>
</ol>

<p>###分类</p>

<ol>
  <li>朴素贝叶斯</li>
  <li>决策树归纳: 通过他的行为来总结他的决策
 找出树根的原则是这一个点挑出来，尽可能消除不确定性。
 Gain（A）= Info - Info（A）
 这就是这个特征为系统带来的信息。</li>
  <li>随机森林</li>
  <li>隐马尔可夫模型
 隐马尔可夫链和贝叶斯网络的模型思维方式接近，区别在于前者更简化，可以看作是后者的一种特例。
 隐马尔可夫模型中，不仅有一串可见状态链，还有一串隐含状态链。对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，进行模拟是相当容易的。但是应用HMM模型的时候，往往缺失一部分信息：</li>
  <li>支持向量机</li>
  <li>遗传算法</li>
</ol>
